{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Data Kuliah\\S2 ITS\\Semester 4\\Seminar 2\\Definition Extraction\\drone-definition\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# Define the columns in the CSV file\n",
    "WORD_COLUMN = 'words'\n",
    "ACTUAL_LABEL_COLUMN = 'actual_class'\n",
    "PREDICTED_LABEL_COLUMN = 'predicted_class'\n",
    "\n",
    "\n",
    "def span_evaluation(filename):\n",
    "    actual_spans = []\n",
    "    predicted_spans = []\n",
    "    predicted_spans_fp = []\n",
    "    model_name = filename.split('/')[1].split('-')[0]\n",
    "    predicted_df = pd.read_csv(filename)\n",
    "    # print(predicted_df)\n",
    "    current_actual_span = []\n",
    "    current_predicted_span = []\n",
    "    current_predicted_span_fp = []\n",
    "    current_predicted_entity = \"\"\n",
    "    for index, row in predicted_df.iterrows():\n",
    "        # reader = csv.DictReader(file)\n",
    "        # print(f\"index: {index}, row: {row}\")\n",
    "        # current_actual_span = []\n",
    "        # current_predicted_span = []\n",
    "        # for row in reader:\n",
    "        word = row[WORD_COLUMN]\n",
    "        actual_label = row[ACTUAL_LABEL_COLUMN]\n",
    "        predicted_label = row[PREDICTED_LABEL_COLUMN]\n",
    "        # print(f\"Word: {word}, Actual: {actual_label}, Predicted: {predicted_label}\")\n",
    "        # predicted_tag = predicted_label[0]\n",
    "        # print(f\"actual_label: {actual_label}, predicted_label: {predicted_label}\")\n",
    "        # Check if it's the beginning of a new entity span\n",
    "        if actual_label.startswith('B-'):\n",
    "            # print(f\"Actual Start-B: {(index, word, actual_label[2:])}\")\n",
    "            if current_actual_span:\n",
    "                actual_spans.append(current_actual_span)\n",
    "                predicted_spans.append(current_predicted_span)\n",
    "                current_actual_span = []\n",
    "                current_predicted_span = []\n",
    "        if predicted_label.startswith('B-'):\n",
    "        #     # print(f\"Predicted Start-B: {(index, word, predicted_label[2:])}\")\n",
    "        #     # print(f\"Found Predicted: {current_actual_span}\")\n",
    "            if current_predicted_span_fp:\n",
    "        #         # actual_spans.append(current_actual_span)\n",
    "                predicted_spans_fp.append(current_predicted_span_fp)\n",
    "        #         # current_actual_span = []\n",
    "                current_predicted_span_fp = []\n",
    "                current_predicted_entity = \"\"\n",
    "            #     predicted_fp_label = predicted_label[2:]\n",
    "            #     current_predicted_entity = predicted_fp_label\n",
    "            #     current_predicted_span_fp.append((index, word, predicted_fp_label))\n",
    "            # else:\n",
    "            #     predicted_fp_label = predicted_label[2:]\n",
    "            #     current_predicted_entity = predicted_fp_label\n",
    "            #     current_predicted_span_fp.append((index, word, predicted_fp_label))\n",
    "        # Check if it's the continuation of an entity span or a non-entity token\n",
    "        if actual_label.startswith(('B-', 'I-')):\n",
    "            # print(f\"Found Actual: {actual_label}\")\n",
    "            current_actual_span.append((index, word, actual_label[2:]))\n",
    "            predicted_label = predicted_label[2:] if predicted_label != 'O' else predicted_label\n",
    "            current_predicted_span.append((index, word, predicted_label))\n",
    "            # print(f\"Current actual: {current_actual_span}\")\n",
    "        if predicted_label.startswith(('B-', 'I-')):\n",
    "        #     # print(f\"Found Predicted: {predicted_label}\")\n",
    "            predicted_fp_label = predicted_label[2:]\n",
    "            if predicted_fp_label == current_predicted_entity:\n",
    "                current_predicted_span_fp.append((index, word, predicted_fp_label))\n",
    "            else:\n",
    "                predicted_spans_fp.append(current_predicted_span_fp)\n",
    "                current_predicted_span_fp = []\n",
    "                current_predicted_entity = predicted_fp_label\n",
    "                current_predicted_span_fp.append((index, word, predicted_fp_label))\n",
    "        #     print(f\"Current predicted: {current_predicted_span}\")\n",
    "\n",
    "    # Append the last span if it exists\n",
    "    if current_actual_span:\n",
    "        actual_spans.append(current_actual_span)\n",
    "        predicted_spans.append(current_predicted_span)\n",
    "    if current_predicted_span_fp:\n",
    "        predicted_spans_fp.append(current_predicted_span_fp)\n",
    "\n",
    "    # Print the constructed entity spans\n",
    "    # print(\"Actual Spans:\")\n",
    "    # for span in actual_spans:\n",
    "    #     print(span)\n",
    "    # print()\n",
    "    # print(\"Predicted Spans:\")\n",
    "    # for span in predicted_spans:\n",
    "    #     print(span)\n",
    "\n",
    "    return actual_spans, predicted_spans, predicted_spans_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_spans, predicted_spans, predicted_spans_fp = span_evaluation('ner_results/xlnet-base-cased/prediction_xlnet-base-cased.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted_spans.txt', 'w') as f:\n",
    "    print(predicted_spans, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_term_tuple(pred_span, actual_spans):\n",
    "    # term1 = [(index1, token1, entity), (index2, token2, entity)]\n",
    "    # term2 = [(index1, token1, entity), (index2, token2, entity)]\n",
    "    pred_token = [t[0] for t in pred_span]\n",
    "    actual_entity = ''\n",
    "    found = False\n",
    "    for actual_span in actual_spans:\n",
    "        actual_token = [t[0] for t in actual_span]\n",
    "        actual_entity = actual_span[0][2]\n",
    "        if pred_token == actual_token:\n",
    "            print(\"Found Mislabeled\\n\")\n",
    "            print(f\"Actual: {actual_span}, Predicted: {pred_span}\")\n",
    "            found = True\n",
    "            continue\n",
    "\n",
    "    return found, actual_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(model_name, actual_spans, predicted_spans, predicted_spans_fp):\n",
    "    # Calculate TP, FP, and FN\n",
    "    # tp = 0\n",
    "    # fp = 0\n",
    "    # fn = 0\n",
    "\n",
    "    # for actual_span, predicted_span in zip(actual_spans, predicted_spans):\n",
    "    #     if actual_span == predicted_span:\n",
    "    #         tp += 1\n",
    "    #     else:\n",
    "            # fn += 1\n",
    "            # fp += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each entity type\n",
    "    entity_types = set()\n",
    "    entity_tp = {}\n",
    "    entity_fp = {}\n",
    "    entity_fn = {}\n",
    "\n",
    "    # Compute the TP and FN\n",
    "    for actual_span, predicted_span in zip(actual_spans, predicted_spans):\n",
    "        actual_entity = actual_span[0][2]\n",
    "        # for term, entity_type in actual_span:\n",
    "        entity_types.add(actual_entity)\n",
    "        \n",
    "        if len(actual_span) != len(predicted_span):\n",
    "            entity_fn[actual_entity] = entity_fn.get(actual_entity, 0) + 1\n",
    "        # elif actual_span == predicted_span:\n",
    "        elif actual_span in predicted_spans:\n",
    "            entity_tp[actual_entity] = entity_tp.get(actual_entity, 0) + 1\n",
    "        else:\n",
    "            if actual_entity == 'FUNCTION':\n",
    "                print(\"Actual: {}, Predicted: {}\".format(actual_span, predicted_span))\n",
    "            entity_fn[actual_entity] = entity_fn.get(actual_entity, 0) + 1\n",
    "\n",
    "    # Compute the FP\n",
    "    print(\"Misclassified Entity\")\n",
    "    for actual_span, predicted_span in zip(actual_spans, predicted_spans):\n",
    "    # for predicted_span in predicted_spans_fp:\n",
    "        predicted_entity_types = set()\n",
    "        predicted_entities = {}\n",
    "        for index, term, entity_type in predicted_span:\n",
    "            # Ignore the 'O' tags\n",
    "            if entity_type in entity_types:\n",
    "                predicted_entity_types.add(entity_type)\n",
    "                predicted_entities[entity_type] = predicted_entities.get(entity_type, 0) + 1\n",
    "        \n",
    "        # Remove the actual entity type from the predicted_entities dict (partial match)\n",
    "        # to prevent the actual entity type being selected in the majority vote\n",
    "        # found, actual_entity = compare_term_tuple(predicted_span, actual_spans)\n",
    "        actual_entity = actual_span[0][2]\n",
    "        # if found and actual_entity in predicted_entities:\n",
    "        if actual_entity in predicted_entities:\n",
    "            del predicted_entities[actual_entity]\n",
    "\n",
    "        # Skip if no predicted entities are valid entity types\n",
    "        if len(predicted_entities) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the majority vote of the predicted entity types\n",
    "        predicted_entity = max(predicted_entities, key=predicted_entities.get)\n",
    "        if predicted_span not in actual_spans:\n",
    "            # if predicted_entity == 'Component':\n",
    "            #     print(\"Predicted: {}\".format(predicted_span))\n",
    "            # print(\"Actual: {}\".format(actual_span))\n",
    "            # print(f\"entity_type: {predicted_entity}\")\n",
    "            entity_fp[predicted_entity] = entity_fp.get(predicted_entity, 0) + 1\n",
    "\n",
    "    # Calculate micro-average precision, recall, and F1-score\n",
    "    micro_tp = sum(entity_tp.values())\n",
    "    micro_fp = sum(entity_fp.values())\n",
    "    micro_fn = sum(entity_fn.values())\n",
    "    # micro_tp = tp\n",
    "    # micro_fp = fp\n",
    "    # micro_fn = fn\n",
    "\n",
    "    micro_precision = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0\n",
    "    micro_recall = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0\n",
    "    micro_f1_score = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "    # micro_f1_swardi = micro_tp / (micro_tp + ((micro_fp + micro_fn) / 2))\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"TP: {micro_tp}\")\n",
    "    print(f\"FP: {micro_fp}\")\n",
    "    print(f\"FN: {micro_fn}\")\n",
    "    # print()\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"TP: {entity_tp}\")\n",
    "    print(f\"FP: {entity_fp}\")\n",
    "    print(f\"FN: {entity_fn}\")\n",
    "    for entity_type in entity_types:\n",
    "        entity_precision = entity_tp.get(entity_type, 0) / (entity_tp.get(entity_type, 0) + entity_fp.get(entity_type, 0)) if (entity_tp.get(entity_type, 0) + entity_fp.get(entity_type, 0)) > 0 else 0\n",
    "        entity_recall = entity_tp.get(entity_type, 0) / (entity_tp.get(entity_type, 0) + entity_fn.get(entity_type, 0)) if (entity_tp.get(entity_type, 0) + entity_fn.get(entity_type, 0)) > 0 else 0\n",
    "        entity_f1_score = (2 * entity_precision * entity_recall) / (entity_precision + entity_recall) if (entity_precision + entity_recall) > 0 else 0\n",
    "\n",
    "        print(f\"Entity Type: {entity_type}\")\n",
    "        print(f\"Precision: {(entity_precision*100):.3f}\")\n",
    "        print(f\"Recall: {(entity_recall*100):.3f}\")\n",
    "        print(f\"F1-Score: {(entity_f1_score*100):.3f}\")\n",
    "        print()\n",
    "    # Investigate the Confusion Matrix\n",
    "    print(f\"Micro-Average Precision: {(micro_precision*100):.3f}\")\n",
    "    print(f\"Micro-Average Recall: {(micro_recall*100):.3f}\")\n",
    "    print(f\"Micro-Average F1-Score: {(micro_f1_score*100):.3f}\")\n",
    "    # print(f\"Micro-Average F1-Sward: {(micro_f1_swardi*100):.3f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: [(36, 'Control', 'FUNCTION'), (37, 'Signal', 'FUNCTION')], Predicted: [(36, 'Control', 'O'), (37, 'Signal', 'ISSUE')]\n",
      "Actual: [(146, 'Aircraft', 'FUNCTION'), (147, 'propulsion', 'FUNCTION'), (148, 'system', 'FUNCTION')], Predicted: [(146, 'Aircraft', 'COMPONENT'), (147, 'propulsion', 'FUNCTION'), (148, 'system', 'FUNCTION')]\n",
      "Actual: [(231, 'automatic', 'FUNCTION'), (232, 'mode', 'FUNCTION')], Predicted: [(231, 'automatic', 'STATE'), (232, 'mode', 'STATE')]\n",
      "Actual: [(285, 'Low', 'FUNCTION'), (286, 'Voltage', 'FUNCTION'), (287, 'Protection', 'FUNCTION')], Predicted: [(285, 'Low', 'ISSUE'), (286, 'Voltage', 'ISSUE'), (287, 'Protection', 'ISSUE')]\n",
      "Actual: [(504, 'Compass', 'FUNCTION'), (505, 'Redundancy', 'FUNCTION'), (506, 'Switch', 'FUNCTION')], Predicted: [(504, 'Compass', 'FUNCTION'), (505, 'Redundancy', 'FUNCTION'), (506, 'Switch', 'COMPONENT')]\n",
      "Actual: [(635, 'write', 'FUNCTION'), (636, 'protection', 'FUNCTION')], Predicted: [(635, 'write', 'O'), (636, 'protection', 'O')]\n",
      "Actual: [(1075, 'tap-zoom', 'FUNCTION')], Predicted: [(1075, 'tap-zoom', 'O')]\n",
      "Actual: [(1114, 'Vision', 'FUNCTION'), (1115, 'sensor(s)', 'FUNCTION')], Predicted: [(1114, 'Vision', 'COMPONENT'), (1115, 'sensor(s)', 'COMPONENT')]\n",
      "Actual: [(1150, 'image', 'FUNCTION'), (1151, 'transmission', 'FUNCTION'), (1152, 'signal', 'FUNCTION')], Predicted: [(1150, 'image', 'ISSUE'), (1151, 'transmission', 'FUNCTION'), (1152, 'signal', 'FUNCTION')]\n",
      "Misclassified Entity\n",
      "TP: 951\n",
      "FP: 29\n",
      "FN: 46\n",
      "Model: xlnet\n",
      "TP: {'COMPONENT': 208, 'STATE': 60, 'PARAMETER': 220, 'FUNCTION': 102, 'ISSUE': 253, 'ACTION': 108}\n",
      "FP: {'ISSUE': 3, 'COMPONENT': 10, 'STATE': 3, 'PARAMETER': 5, 'FUNCTION': 7, 'ACTION': 1}\n",
      "FN: {'FUNCTION': 9, 'COMPONENT': 5, 'ACTION': 16, 'STATE': 4, 'ISSUE': 9, 'PARAMETER': 3}\n",
      "Entity Type: ISSUE\n",
      "Precision: 98.828\n",
      "Recall: 96.565\n",
      "F1-Score: 97.683\n",
      "\n",
      "Entity Type: STATE\n",
      "Precision: 95.238\n",
      "Recall: 93.750\n",
      "F1-Score: 94.488\n",
      "\n",
      "Entity Type: ACTION\n",
      "Precision: 99.083\n",
      "Recall: 87.097\n",
      "F1-Score: 92.704\n",
      "\n",
      "Entity Type: COMPONENT\n",
      "Precision: 95.413\n",
      "Recall: 97.653\n",
      "F1-Score: 96.520\n",
      "\n",
      "Entity Type: FUNCTION\n",
      "Precision: 93.578\n",
      "Recall: 91.892\n",
      "F1-Score: 92.727\n",
      "\n",
      "Entity Type: PARAMETER\n",
      "Precision: 97.778\n",
      "Recall: 98.655\n",
      "F1-Score: 98.214\n",
      "\n",
      "Micro-Average Precision: 97.041\n",
      "Micro-Average Recall: 95.386\n",
      "Micro-Average F1-Score: 96.206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "filenames = ['ner_results/xlnet-base-cased/prediction_xlnet-base-cased.csv',\n",
    "            #  'ner_results/bert-base-cased/prediction_bert-base-cased.csv',\n",
    "            #  'ner_results/distilbert-base-cased/prediction_distilbert-base-cased.csv',\n",
    "            #  'ner_results/distilroberta-base/prediction_distilroberta-base.csv',\n",
    "            #  'ner_results/electra-base/prediction_electra-base-discriminator.csv',\n",
    "            #  'ner_results/roberta/prediction_roberta-base.csv',\n",
    "             ]\n",
    "\n",
    "for filename in filenames:\n",
    "    actual_spans, predicted_spans, predicted_spans_fp = span_evaluation(filename)\n",
    "    model_name = filename.split('/')[1].split('-')[0]\n",
    "    compute_score(model_name, actual_spans, predicted_spans, predicted_spans_fp)\n",
    "    # print(f\"Actual:\\n {actual_spans}\")\n",
    "    # span_evaluation('ner_results/xlnet-base-cased/prediction_xlnet-base-cased.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model   TP   FP   FN  Precision  Recall      F1\n",
      "0          xlnet  951   27   46     97.239  95.386  96.304\n",
      "1           bert  949   30   48     96.936  95.186  96.053\n",
      "2     distilbert  944   36   53     96.327  94.684  95.498\n",
      "3  distilroberta  945   30   52     96.923  94.784  95.842\n",
      "4        electra  827  135  170     85.967  82.949  84.431\n",
      "5        roberta  947   27   50     97.228  94.985  96.093\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Experiment results\n",
    "results = [\n",
    "    {\n",
    "        'Model': 'xlnet',\n",
    "        'TP': {'COMPONENT': 208, 'STATE': 60, 'PARAMETER': 220, 'FUNCTION': 102, 'ISSUE': 253, 'ACTION': 108},\n",
    "        'FP': {'ISSUE': 3, 'COMPONENT': 10, 'STATE': 2, 'PARAMETER': 5, 'FUNCTION': 6, 'ACTION': 1},\n",
    "        'FN': {'FUNCTION': 9, 'COMPONENT': 5, 'ACTION': 16, 'STATE': 4, 'ISSUE': 9, 'PARAMETER': 3},\n",
    "        'Precision': 97.239,\n",
    "        'Recall': 95.386,\n",
    "        'F1': 96.304\n",
    "    },\n",
    "    {\n",
    "        'Model': 'bert',\n",
    "        'TP': {'COMPONENT': 208, 'STATE': 60, 'PARAMETER': 218, 'FUNCTION': 102, 'ACTION': 108, 'ISSUE': 253},\n",
    "        'FP': {'ISSUE': 5, 'FUNCTION': 9, 'COMPONENT': 11, 'STATE': 1, 'PARAMETER': 2, 'ACTION': 2},\n",
    "        'FN': {'FUNCTION': 9, 'ISSUE': 9, 'ACTION': 16, 'STATE': 4, 'PARAMETER': 5, 'COMPONENT': 5},\n",
    "        'Precision': 96.936,\n",
    "        'Recall': 95.186,\n",
    "        'F1': 96.053\n",
    "    },\n",
    "    {\n",
    "        'Model': 'distilbert',\n",
    "        'TP': {'COMPONENT': 207, 'STATE': 60, 'FUNCTION': 99, 'PARAMETER': 218, 'ISSUE': 253, 'ACTION': 107},\n",
    "        'FP': {'FUNCTION': 9, 'COMPONENT': 10, 'ACTION': 2, 'STATE': 3, 'ISSUE': 6, 'PARAMETER': 6},\n",
    "        'FN': {'ISSUE': 9, 'ACTION': 17, 'STATE': 4, 'FUNCTION': 12, 'PARAMETER': 5, 'COMPONENT': 6},\n",
    "        'Precision': 96.327,\n",
    "        'Recall': 94.684,\n",
    "        'F1': 95.498\n",
    "    },\n",
    "    {\n",
    "        'Model': 'distilroberta',\n",
    "        'TP': {'COMPONENT': 208, 'STATE': 60, 'PARAMETER': 217, 'FUNCTION': 100, 'ACTION': 108, 'ISSUE': 252},\n",
    "        'FP': {'ISSUE': 6, 'COMPONENT': 9, 'FUNCTION': 9, 'STATE': 5, 'ACTION': 1},\n",
    "        'FN': {'FUNCTION': 11, 'ISSUE': 10, 'ACTION': 16, 'STATE': 4, 'PARAMETER': 6, 'COMPONENT': 5},\n",
    "        'Precision': 96.923,\n",
    "        'Recall': 94.784,\n",
    "        'F1': 95.842\n",
    "    },\n",
    "    {\n",
    "        'Model': 'electra',\n",
    "        'TP': {'COMPONENT': 173, 'PARAMETER': 180, 'FUNCTION': 94, 'ISSUE': 231, 'STATE': 51, 'ACTION': 98},\n",
    "        'FP': {'ISSUE': 69, 'PARAMETER': 14, 'COMPONENT': 16, 'FUNCTION': 19, 'ACTION': 14, 'STATE': 3},\n",
    "        'FN': {'STATE': 13, 'FUNCTION': 17, 'COMPONENT': 40, 'ACTION': 26, 'ISSUE': 31, 'PARAMETER': 43},\n",
    "        'Precision': 85.967,\n",
    "        'Recall': 82.949,\n",
    "        'F1': 84.431\n",
    "    },\n",
    "    {\n",
    "        'Model': 'roberta',\n",
    "        'TP': {'COMPONENT': 211, 'STATE': 60, 'FUNCTION': 101, 'PARAMETER': 215, 'ISSUE': 252, 'ACTION': 108},\n",
    "        'FP': {'COMPONENT': 11, 'FUNCTION': 9, 'STATE': 2, 'ISSUE': 2, 'PARAMETER': 1, 'ACTION': 2},\n",
    "        'FN': {'ACTION': 16, 'STATE': 4, 'FUNCTION': 10, 'ISSUE': 10, 'PARAMETER': 8, 'COMPONENT': 2},\n",
    "        'Precision': 97.228,\n",
    "        'Recall': 94.985,\n",
    "        'F1': 96.093\n",
    "    }\n",
    "]\n",
    "\n",
    "# Recap into pandas DataFrame\n",
    "recap_data = []\n",
    "for result in results:\n",
    "    tp_sum = sum(result['TP'].values())\n",
    "    fp_sum = sum(result['FP'].values())\n",
    "    fn_sum = sum(result['FN'].values())\n",
    "    \n",
    "    recap_data.append({\n",
    "        'Model': result['Model'],\n",
    "        'TP': tp_sum,\n",
    "        'FP': fp_sum,\n",
    "        'FN': fn_sum,\n",
    "        'Precision': result['Precision'],\n",
    "        'Recall': result['Recall'],\n",
    "        'F1': result['F1']\n",
    "    })\n",
    "\n",
    "recap_df = pd.DataFrame(recap_data)\n",
    "\n",
    "# Display the recap DataFrame\n",
    "print(recap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entity Type   TP  FP  FN Precision  Recall F1-Score\n",
      "0    FUNCTION  102   6   9    94.444  91.892   93.151\n",
      "1   COMPONENT  208  10   5    95.413  97.653   96.520\n",
      "2   PARAMETER  220   5   3    97.778  98.655   98.214\n",
      "3      ACTION  108   1  16    99.083  87.097   92.704\n",
      "4       ISSUE  253   3   9    98.828  96.565   97.683\n",
      "5       STATE   60   2   4    96.774  93.750   95.238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Experiment results\n",
    "results = [\n",
    "    {\n",
    "        'Model': 'xlnet',\n",
    "        'TP': {'COMPONENT': 208, 'STATE': 60, 'PARAMETER': 220, 'FUNCTION': 102, 'ISSUE': 253, 'ACTION': 108},\n",
    "        'FP': {'ISSUE': 3, 'COMPONENT': 10, 'STATE': 2, 'PARAMETER': 5, 'FUNCTION': 6, 'ACTION': 1},\n",
    "        'FN': {'FUNCTION': 9, 'COMPONENT': 5, 'ACTION': 16, 'STATE': 4, 'ISSUE': 9, 'PARAMETER': 3},\n",
    "        'Precision': 97.239,\n",
    "        'Recall': 95.386,\n",
    "        'F1-Score': 96.304\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize lists to store recap data\n",
    "recap_data = []\n",
    "entity_types = ['FUNCTION', 'COMPONENT', 'PARAMETER', 'ACTION', 'ISSUE', 'STATE']\n",
    "\n",
    "# Iterate over entity types\n",
    "for entity_type in entity_types:\n",
    "    # Extract TP, FP, and FN values from the dictionaries\n",
    "    tp = results[0]['TP'].get(entity_type, 0)\n",
    "    fp = results[0]['FP'].get(entity_type, 0)\n",
    "    fn = results[0]['FN'].get(entity_type, 0)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Append recap data for the entity type\n",
    "    recap_data.append({\n",
    "        'Entity Type': entity_type,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Precision': (precision*100),\n",
    "        'Recall': recall*100,\n",
    "        'F1-Score': f1_score*100\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(recap_data)\n",
    "for column in df.columns[4:]:\n",
    "    df[column] = df[column].map('{:.3f}'.format)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
